# Autoencoder and GAN Implementation

This repository contains two source codes for implementing an Autoencoder and a Generative Adversarial Network (GAN) for image-related tasks. The source codes are as follows:

## **1. `test_utils.py`**

The **`test_utils`** folder includes helper functions that are used in the implementation of the Autoencoder and GAN models. These functions provide various utility operations for data preprocessing, visualization, and evaluation.

## **2. `01_CustomAE.ipynb`**

The **`01_CustomAE.ipynb`** notebook contains the implementation of an Autoencoder neural network from scratch using NumPy. The Autoencoder is trained on the MNIST dataset, which consists of handwritten digit images. The notebook demonstrates how to construct the Autoencoder model, preprocess the data, and train the model using the MNIST dataset. It also includes visualizations of the reconstructed images.

## **3. `02_PyTorchGAN.ipynb`**

The **`02_PyTorchGAN.ipynb`** notebook presents the implementation of a Generative Adversarial Network (GAN) using PyTorch. The GAN is trained on the MNIST dataset to generate new images that resemble handwritten digits. The notebook demonstrates how to define the GAN architecture, preprocess the data, train the GAN model, and visualize the generated images. It also provides explanations of the loss function used and discusses different GAN types.

Both notebooks provide step-by-step instructions, code explanations, and visualizations to help you understand the concepts and implementation details. They serve as a practical guide for training the Autoencoder and GAN models on the MNIST dataset.

Feel free to explore the notebooks and experiment with the code to gain insights into Autoencoder and GAN architectures and their applications in image generation tasks.

Please note that the notebooks assume basic knowledge of neural networks, image processing, and programming concepts.

# **Visual Results**

This section presents the visual results obtained from the implementation of the Autoencoder and GAN models on the MNIST dataset.

## **Autoencoder Output Visualization**

The following images showcase the outputs generated by the Autoencoder model trained on the MNIST dataset. These images represent the reconstructed versions of the input digits.

![Table 1: Visualization of Autoencoder Training Process](Autoencoder%20and%20GAN%20Implementation%20bddc88d9b0e94f48b2860cd01971ff94/01.png)

Table 1: Visualization of Autoencoder Training Process

## **GAN Generated Digit Visualization**

The GAN model trained on the MNIST dataset is capable of generating new digit images. The following grid displays a set of generated digits after each epoch of training.

![Table 2: Visualization of the Generator’s Training Process](Autoencoder%20and%20GAN%20Implementation%20bddc88d9b0e94f48b2860cd01971ff94/02.png)

Table 2: Visualization of the Generator’s Training Process

The visual results provide insights into the performance and quality of the Autoencoder and GAN models. They demonstrate the effectiveness of the models in reconstructing input images and generating new digit samples.

In the beginning, the generator produced almost pure noise since it does not know anything about the dataset. We could not talk about any quality or diversity.
After a while, the generator could produce different outputs. For example, Epoch 30 had
{0,3,4,5,7,8,9}. We could state it has diversity to generate output and its quality is indisputably
improved.
At the end of the training process, the generator produces similar outputs: Most outputs look like 9. This failure is called ‘Mode Collapse’. This is because the discriminator is stuck in the local minimum and could not find the way out of the trap. So, the generator decreases its diversity and just produces a small set of output that the discriminator could not distinguish.